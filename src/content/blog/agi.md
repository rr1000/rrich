---
title: "AGI"
date: 2026-01-28
description: "The definition problem isn't semantic. It's the whole game."
---

AGI has become a useless term. 

Every few months someone announces we're "basically there" or "95% of the way." The goalposts move depending on who's talking and what they're selling. VCs say we're close because it helps their portfolio narrative. Researchers say we're far because it helps their funding narrative. Everyone's probably wrong in different directions.

The problem is the definition. And the definition problem isn't semantic. It's the whole game.

## The bar is too low

If AGI means "can perform tasks a human can perform," then current LLMs are arguably already AGI. They write essays, answer questions, code, do math, pass professional exams. By the task-performance definition, we're done. Pop the champagne.

But that feels wrong. Something is obviously missing. ChatGPT passing the bar exam doesn't feel like the same category of event as "man's last invention." The word AGI should mean something bigger than "surprisingly good autocomplete."

The task-performance definition fails because it only measures outputs. It doesn't ask how those outputs were produced, or whether the underlying process could generalize to genuinely novel problems. Current models are very good at sophisticated retrieval and recombination of their training data. They're bad at everything else.

## The bar is too high

The other extreme: AGI as the system that takes over the world overnight. Recursive self-improvement, rapid capability gain, escapes human control, harnesses all computing power within days. Skynet, basically.

By this definition we're nowhere close and might never be. Current models can't inspect their own weights. They can't redesign themselves. They can't even remember yesterday's conversation. The gap between "helpful chatbot" and "autonomous world-altering superintelligence" is so vast that using the same term for both feels like a category error.

This definition is also suspiciously unfalsifiable. If nothing dramatic happens, AGI hasn't arrived yet. If something dramatic happens, well, we told you so.

## A better bar

Here's what I think actually matters: can it push the frontier?

Not "can it do tasks humans have done." Can it do things humans haven't done yet? Can it produce genuinely novel ideas? Can it solve open problems, not just closed ones?

Curing cancer would count. Not because cancer is special, but because nobody's done it yet. A system that could actually make progress on unsolved problems (in science, mathematics, engineering, whatever) would be qualitatively different from current AI. That's the discontinuity worth naming.

Current models operate within existing knowledge. They're very good at it. But they can't extend the boundary. An LLM can explain how CRISPR works because people wrote papers about CRISPR. It can't figure out a novel application nobody's thought of. It's downstream of human discovery, not upstream.

## The meta-capabilities

Maybe the real definition isn't about any specific task at all. Maybe it's about capabilities that enable other capabilities:

Can it learn in real-time from experience, without retraining?

Can it recognize the limits of its own knowledge and go fill the gaps?

Can it set goals and pursue them autonomously over time?

Can it transfer genuine understanding to domains it wasn't explicitly trained on?

Current models fail all of these. They can't learn from conversation. They can't notice they're weak at biology and go study it. They can't decide to run an experiment and be surprised by the result. Every interaction is isolated, amnesiac, dependent on whatever humans feed them.

Without those meta-capabilities, you don't have general intelligence. You have a very sophisticated tool. Useful, but fundamentally limited in ways that matter.

## Why this matters

The definition question isn't academic. It determines what we're building toward, what we're worried about, and how we allocate resources.

If AGI is "task performance," then we're almost done and the remaining work is incremental. Scale up, fine-tune, polish the edges.

If AGI is "can push the frontier," then we're missing something fundamental and more compute won't fix it. We need new architectures, new training paradigms, maybe new ideas we haven't had yet.

If AGI is "autonomous world-altering superintelligence," then either we're facing an existential risk that deserves more attention than it's getting, or we're chasing a science fiction scenario that will never materialize. Hard to plan around that uncertainty.

I lean toward the middle definition. The thing worth calling AGI is the thing that can do what humans can't yet do. Novel discovery, genuine insight, expanding the boundary of knowledge rather than operating within it.

By that definition, we're not close. The current paradigm (transformer architectures, next-token prediction, training on human-generated text) might not be capable of it at all. Or maybe it is and we just need more scale. Honestly, nobody knows.

But at least the definition gives us something to aim at.
